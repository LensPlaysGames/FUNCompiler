\documentclass[12pt]{report}

\newcommand{\langname}{Funguwuage}
\newcommand{\lang}{\texttt{\langname} }

\title  {\langname\ Developer's Manual}
\author {Rylan Lens Kellogg}

\newlength{\inprogramwidth}
\settowidth{\inprogramwidth}{A few lines of text in a block.}

% Easy, interactive cross-references with '\hyperref[label]{text}'.
\usepackage{hyperref}
% Prevent section titles from appearing at the bottom of pages!
\usepackage[nobottomtitles, nobottomtitles*]{titlesec}
% Fancy verbatim markup, for better source code.
\usepackage{fancyvrb}
% Fancy headers, for chapter title page headers
\usepackage{fancyhdr}
% Easy flow charts and other simple diagrams
\usepackage{smartdiagram}
% Very complicated diagram drawing
\usepackage{tikz}
\usetikzlibrary{arrows, shapes}

% \titleformat{<command>}[<shape>]
% {<format>}
% {<label>}
% {<sep>}
% {<before-code>}
% {<after-code>}

% Remove numbering from chapter heading.
%\titleformat  {\chapter} [hang]
%{ \normalfont\huge\bfseries }
%{}{ 0pt }
%{  }
%{  }
% Make chapter heading snug up to top of page.
\titlespacing*{\chapter}{-21pt}{-72pt}{10pt}

% Remove numbering from section heading.
%\titleformat  {\section} [hang]
%{ \normalfont\Large\bfseries }
%{}{ 0pt }
%{  }
%{  }
% Make section heading have a sensible amount of spacing around it.
\titlespacing*{\section}{-10pt}{24pt}{10pt}

% Set variable paragraph skip (vertical blank space between paragraphs).
\setlength{\parskip}{3pt plus 3pt minus 1pt}


% I should probably just use geometry, but this works so...

% Move fancy header upwards. It's like halfway down the page by default.
\setlength{\headheight}{14.5pt}
\addtolength{\topmargin}{-64pt}
% Extend text downwards, it stops with like 3 inches on the bottom.
\addtolength{\textheight}{128pt}

% Smart diagram style(s)
\smartdiagramset{
  monotone/.style={
    uniform arrow color   = true,
    arrow color           = black,
    uniform color list    = white!60 for 20 items,
    back arrow disabled   = true
  }
}

\begin{document}

% I have no idea how this works but it puts the chapter name at the top of each page.
% https://stackoverflow.com/a/48735234
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{#1}{#1}}
\fancyhf{}
\fancyhead[C]{\leftmark}
\fancyfoot[C]{\thepage}

% Title Page
\hypersetup{pageanchor=false}
\begin{titlepage}
  \maketitle
\end{titlepage}

\chapter{Overview}
\label{ch:Overview}

Currently, \lang has three stages (or phases, or levels, anything to that regard; we'll call them stages).

\begin{center}
  \smartdiagramset{
    monotone,
    text width = 4cm,
    module x sep = 5cm,
    font = \large
  }
  \smartdiagram[flow diagram:horizontal]{
    Parsing,
    Type-checking,
    Code Generation
  }
\end{center}

The parser converts \lang source code into a data structure that represents the program in it's entirety.

The type-checker then validates this data structure, ensuring that the semantics of types are being followed properly.

The code generation stage creates a new data structure (an in-memory intermediate representation) that follows static single assignment form.
This intermediate representation gets converted by each backend into
architecture specific implementations (machine code, asm, etc).

\chapter{Parsing}
\label{ch:parsing}

The parsing stage validates and begins to understand the syntax of \lang source code.

The parsing stage can be thought of as two systems that work together: the \emph{lexer} and the \emph{parser}. The parser drives the parsing stage, and calls to the lexer as needed.

The parser keeps track of all state (how much has been parsed so far, etc). The lexer is a simple tool to increment the state of the parser. The parsing state state consists of a lexeme, it's length in bytes, and a pointer to the end of the lexeme (where we will begin lexing from next). This pointer starts at the beginning of our source code, and makes it's way towards the end as the parser advances the parsing state.

\section{Lexer}
\label{subsec:lexer}

The lexer's job is to split up the input source code into understandable chunks (called lexemes); after all, the file is read as one, large, arbitrary sequence of bytes. The lexer splits these bytes into understandable pieces, and also strips off parts of the file that aren't needed, like whitespace and comments.

The lexer must not lex more than one lexeme at a time. It is the job of the parser to understand sequences of lexemes.

\vspace{1em}

There are two main actions the lexer can make:
\begin{itemize}
\item[advance] Get the lexeme directly after the given position, updating position to end of new lexeme.
\item[expect]  Get the lexeme directly after the given position, only updating position if the given lexeme was found.
\end{itemize}

\begin{minipage}{\textwidth}

Given the following source code:
\begin{Verbatim}[samepage=true]
  ;; Intercept Simple Lexing Example
  a : integer
  a := 69
\end{Verbatim}

The lexer would produce the following lexemes (line separated) from the above source code, on subsequent calls.
\begin{Verbatim}[samepage=true]
  a
  :
  integer
  a
  :
  =
  69
\end{Verbatim}
\end{minipage}

\section{Parser}
\label{subsec:parser}

The parser is the main component of the parsing stage, and does most of the heavy lifting.

The job of the parser is to create an understanding of the program, from the compiler's point of view. To do this, the program must be unambigous, and contain only sequences of lexemes that are understood.

One thing you may be wondering: how does a computer program (the compiler) \emph{know} something? How can you make it \emph{understand}? The idea is that by defining a data structure in our compiler that can store the meaning of programs, as well as any necessary data along with it, the compiler can construct one of these structures and end up with an understood program, because the data structure is already understood.

There are two usual choices for data structures that can fully define the semantics of a program, while also storing data along with it.

One, the most common, is called an \emph{abstract syntax tree}, or AST. The AST is a fancy name for a tree that houses only the bare minimum data of what is needed for the program moving forward. For example, parentheses aren't needed in an AST, because they don't actually do anything. They are only for the parser to understand what order to parse things in; since we don't need them after that (the AST is already constructed in the right order), they don't get included in the AST itself. This is why it is an \emph{abstract} syntax tree; the tree doesn't map one-to-one with the source code syntax. Some compilers do build what's called a parse tree that does map one-to-one with the source code.

Another, still fairly common, is called a \emph{directed acyclic graph}, or DAG. A DAG is much like an AST, except that it is not a tree structure, but a graph. This means any node can be connected to any node, and lots of complicated group and category theory ensues. Because a single node can be referenced by any other node, it does mean that it takes less memory to store the same program, and is likely more efficient overall. The problem lies within it's complexity: DAGs are hard to manage, and when it goes wrong, it goes very, very wrong. However, it is clear that a DAG is better in almost every way than an AST, if you are willing to deal with the headaches.

For the sake of simplicity and understanding, \lang uses an AST, or abstract syntax tree, and generates it from recognized sequences of lexemes.

A tree, in computer programming, is a data structure that can branch into multiple data structures. An AST happens to be a recursive tree, in that each of the branches of the tree can have branches, and those branches can have branches, and so on. A lot of compilers use a binary tree, or btree, which is a tree with a recursive left hand side and right hand side. The problem you see in using binary trees is that it is rather difficult to represent anything with more than two children (as one might expect). This is quite a common case in programming language ASTs; for example, when calling a function, there may be any amount of arguments. Most compilers go the LISP route, and have these sort of lists implemented as recursive pairs, where as long as the lhs is another pair, the list continues. I feel like this is a messy implementation, and results in code that is quite hard to read, due to chains of dereferences, i.e. \verb|lhs->lhs->rhs->value|.

The compiler includes a text-debug format to print the abstract syntax tree that is produced, and can be printed out for any program by adding the verbose command line flag: \verb|-v|.
% NOTE: It would be cool to output this printout into a file and have our own little file format to save this intermediate step. This would help a load if we wanted to start doing conditional compilation.

\begin{minipage}{\textwidth}

Let's take a look at an example. Given the following source code:
\begin{Verbatim}[samepage=true]
  ;; Intercept Parsing Example
  fact : integer (n : integer) = integer (n : integer) {
    if n < 2 {
      1
    } else {
      n * fact(n - 1)
    }
  }

  fact(5)
\end{Verbatim}

The parser would produce the following abstract syntax tree:
\begin{Verbatim}
  PROGRAM
    VARIABLE DECLARATION
      SYM:"fact"          <- colon separates node type from node value
    VARIABLE REASSIGNMENT
      SYM:"fact"
      FUNCTION
        SYM:"integer" (0) <- refers to level of pointer indirection
        NONE              <- empty node to store list of parameter types
          SYM:"integer"
        NONE              <- empty node to store list of body expressions
          IF
            BINARY OPERATOR:"<"
              VARIABLE ACCESS:"n"
              INT:2
            NONE          <- stores 'then' body
              INT:1
            NONE          <- stores 'otherwise' body
              BINARY OPERATOR:"*"
                VARIABLE ACCESS:"n"
                FUNCTION CALL
                  SYM:"fact"
                  BINARY OPERATOR:"-"
                    VARIABLE ACCESS:"n"
                    INT:1
    FUNCTION CALL
      SYM:"fact"
      INT:5
\end{Verbatim}

As you can see, this is a complete representation of the program. It is able to understand high level concepts that the programmer construed in the source code, like conditional control flow, variable accesses, function calls with arguments, and more.

\end{minipage}


\chapter{Type-checking}
\label{ch:typechecking}

Type-checking refers to the process of ensuring the types of expressions are what they are expected to be.

This is only relevant due to \lang being statically typed. This means the type of a variable is known at compile-time, due to the programmer declaring it. This declared type is associated with the variable, and any expressions assigned to it must return a compatible type. It also comes into play with binary expressions: they operate on two objects, each of some type. It wouldn't make any sense to try to add a function to an array, would it? Function calls' arguments must match a function's declared parameters' types. A dereference may only operate on a pointer. All of these semantics of the language are enforced by the type-checker.

\chapter{Code Generation}
\label{ch:codegen}

Finally, we arrive at code generation. This is definitely the most complicated part, and is where compiler developers get lost for decades. While \lang started as a very simple, x86\_64-only compiler, it is slowly growing to become a compiler that supports many backends, each of which cross platform themselves.

Thanks to Sirraide, our compiler has an intermediate representation. I didn't budge easy, but he was insistent and convincing.

If you are like me, you may wonder why this IR is even needed; it seems (at least to me) like it is of no use, and everything it represents could be stored in the AST. If you are not like me, that's good. You're probably smart :P.

Here is a list of reasons that were used to convince myself of using an IR:
- There are none.

Seriously, you can do everything with just an AST, I promise. If you don't want an IR, don't use one. In the early versions of the compiler (that I wrote), there was no IR and codegen happened straight from AST. It wasn't always optimal but through peephole optimizations I'm fairly sure we could get it very good. In any case, Sirraide insisted and made the PR, and I won't turn down a chance to learn something I don't know.

First of all, in the intermediate representation, that doesn't conform to any hardware standard, there are some assumptions that can be made. If there are no hardware limitations, then every value could just be stored in a new register; why wouldn't there be infinite? With this, we would never have to worry about slow-downs from RAM access or an appalling disk IO situation: it would simply always be register to register movement, and only when the user dereferences something would we begin to have memory accesses generated. It's like a code optimizers wet dream: everything in a register, and only explicit memory accesses.

% END INTRO

\begin{Verbatim}[samepage = true]
  ;; Intercept Intermediate Representation Example
  ;; Expressions are annotated with numbers for easy referencing.

  ;; Let us assume that input_condition and if_condition are somehow
  ;; different every time the program runs, like parameters.
  input_condition : integer           ; 0
  if_condition : integer              ; 1

  a : integer                         ; 2
  b : integer = 420                   ; 3
  c : integer = 69                    ; 4
  d : integer                         ; 5
  e : integer                         ; 6
  f : integer                         ; 7

  while 1 {                           ; 8, 9

    a := b + c                        ; 10
    d := a * -1                       ; 11
    e := d + f                        ; 12

    if input_condition {              ; 13, 14
      f := 2 * e                      ; 15
    } else {
      b := d + e                      ; 16
      e := e - 1                      ; 17
    }

    b := f + c                        ; 18
  }
\end{Verbatim}

And the generated AST of the program:
\begin{Verbatim}
  PROGRAM
    VARIABLE DECLARATION:"input_condition"    ; 0
    VARIABLE DECLARATION:"if_condition"       ; 1
    VARIABLE DECLARATION:"a"                  ; 2
    VARIABLE DECLARATION:"b"                  ; 3
    VARIABLE ASSIGNMENT
      SYM:"b"
      INT:420
    VARIABLE DECLARATION:"c"                  ; 4
    VARIABLE ASSIGNMENT
      SYM:"c"
      INT:69
    VARIABLE DECLARATION:"d"                  ; 5
    VARIABLE DECLARATION:"e"                  ; 6
    VARIABLE DECLARATION:"f"                  ; 7
    WHILE                                     ; 8
      INT:1                                   ; 9
      NONE
        VARIABLE ASSIGNMENT                   ; 10
          SYM:"a"
          BINARY OPERATOR:"+"
            VARIABLE ACCESS:"b"
            VARIABLE ACCESS:"c"
        VARIABLE ASSIGNMENT                   ; 11
          SYM:"d"
          BINARY OPERATOR:"*"
            VARIABLE ACCESS:"a"
            INT:-1
        VARIABLE ASSIGNMENT                   ; 12
          SYM:"e"
          BINARY OPERATOR:"+"
            VARIABLE ACCESS:"d"
            VARIABLE ACCESS:"f"
        IF                                    ; 13
          VARIABLE ACCESS:"input_condition"   ; 14
          NONE
            VARIABLE ASSIGNMENT               ; 15
              SYM:"f"
              BINARY OPERATOR:"*"
                INT:2
                VARIABLE ACCESS:"e"
          NONE
            VARIABLE ASSIGNMENT               ; 16
              SYM:"b"
              BINARY OPERATOR:"+"
                VARIABLE ACCESS:"d"
                VARIABLE ACCESS:"e"
            VARIABLE ASSIGNMENT               ; 17
              SYM:"e"
              BINARY OPERATOR:"-"
                VARIABLE ACCESS:"e"
                INT:1
        VARIABLE ASSIGNMENT                   ; 18
          SYM:"b"
          BINARY OPERATOR:"+"
            VARIABLE ACCESS:"f"
            VARIABLE ACCESS:"c"
\end{Verbatim}

Previously, this AST would be used to generate code by the code generation backend (the frontend delegates to different backends based on defaults or configuration at the command line). However, this now gets converted into the following IR (excluding initial variable declarations, only for the sake of brevity in the initial basic block):

Here is the while loop body from our current program, in terms of \lang's intermediate representation, or IR.
\begin{center}
  \begin{tikzpicture}[
    > = latex', auto,
    block/.style = {
      rectangle,
      draw=black,
      thick,
      align=flush center,
      rounded corners,
      minimum height=4em
    },]
\matrix [column sep=5mm,row sep=7mm]{
  % row 1
  &
  \node [block, text width=\inprogramwidth] (firstbox) {
    a := b + c    ; 10

    d := a * -1    ; 11

    e := d + f    ; 12
  }; & \\
  % row 2
  & \node[block] (if) {
    if input\_condition ; 13, 14
  }; & \\
  \node [block] (b1) {
    f := 2 * e    ; 15

  }; &
  & \node [block] (b2) {
    b := d + e    ; 16
    \\
    e := e - 1    ; 17
  };\\
  & \node [block] (d1) {
    b := f + c    ; 18
  };\\
};

\draw[->] (firstbox) -- (if);
\draw[->] (if) -| (b1);
\draw[->] (if) -| (b2);
\draw[->] (b1) -v (d1);
\draw[->] (b2) -v (d1);
% Arrow with label
% \draw[->] (b2) -v (d1) node[midway,above left] {-10};

\draw[-latex,black] ($(d1.east) + (.4,0)$) arc
    [
        start angle = 90,
        end angle =   -80,
        x radius =    6cm,
        y radius =    -3.6cm
    ] ;
\end{tikzpicture}
\end{center}

Each box with code within it is referred to as a \emph{basic block}. \\
The diagram above illustrates the basic blocks of the "main" \emph{function} in our example program and their relation to one another.

\begin{itemize}
\item[Basic Block]
  A flat list of IR instructions that must end in a branch.
\item[Function]
  A flat list of basic blocks with at least an entry and a return block specified, although they may be the same.
\end{itemize}

However, this diagram does not actually conform to \lang's intermediate representation; there is one more step to complete before it matches one-to-one: conversion into static single assignment form, or SSA form for short.

\section{Static Single Assignment Form}
\label{sec:codegen-ssa}

A variable has different values at different times throughout the program, all with different lifetimes... They almost sound like different variables! Wait... what if they were?

Each assignment of a variable after the first creating a new variable is the entire idea behind \emph{SSA}, or static single assignment, form. SSA form is a form of program in which each variable does not get assigned more than one time.

Let's take a look at the example above, converted into SSA form.
\begin{center}
  \begin{tikzpicture}[
    > = latex', auto,
    block/.style = {
      rectangle,
      draw=black,
      thick,
      align=flush center,
      rounded corners,
      minimum height=4em
    },]
\matrix [column sep=5mm,row sep=7mm]{
  % row 1
  &
  \node [block, label = above:{\small bb0}, text width=\inprogramwidth] (firstbox) {
    $a_1$ := b + c    ; 10

    $d_1$ := $a_1$ * -1    ; 11

    $e_1$ := $d_1$ + f    ; 12
  }; & \\
  % row 2
  & \node[block, label = below:{\small bb1}] (if) {
    if input\_condition ; 13, 14
  }; & \\
  \node [block, label = left:{\small bb2}] (b1) {
    f := 2 * $e_1$    ; 15

  }; &
  & \node [block, label = left:{\small bb3}] (b2) {
    $b_1$ := $d_1$ + $e_1$    ; 16
    \\
    $e_2$ := $e_1$ - 1    ; 17
  };\\
  & \node [block, label = above:{\small bb4}] (d1) {
    $b_2$ := f + c    ; 18
  };\\
};

\draw[->] (firstbox) -- (if);
\draw[->] (if) -| (b1);
\draw[->] (if) -| (b2);
\draw[->] (b1) -v (d1);
\draw[->] (b2) -v (d1);
% Arrow with label
% \draw[->] (b2) -v (d1) node[midway,above left] {-10};

\draw[-latex,black] ($(d1.east) + (.4,0)$) arc
    [
        start angle = 90,
        end angle =   -81,
        x radius =    6.8cm,
        y radius =    -4.18cm
    ] ;
\end{tikzpicture}
\end{center}

% TODO: They are up above, but maybe should be here? Basic block, function, value, etc. vocabulary definitions...

The above diagram is not what the actual data structure appears as, so let's take a look at a lower level representation of the intermediate representation. How about that?

\begin{itemize}
\item[\%]
  A temporary value, like a register on a CPU.
\item[bb]
  A basic block within a function.
\item[fun]
  A function with a list of basic blocks.
\end{itemize}


\begin{Verbatim}[samepage = true]
fun main:
  bb0:
    %r0 = global_load("b")
    %r1 = global_load("c")
    %r2 = add(%r0, %r1)
    global_store(%r2, "a") ;;  10
    %r3 = global_load("a")
    %r4 = immediate(-1)
    %r5 = multiply(%r3, %r4)
    global_store(%r5, "d") ;; 11
    ;; ...
\end{Verbatim}

% TODO: There has to be a better title than this!
\section{Variable Liveness}
\label{sec:codegen-variable-liveness}

WARNING: The following section contains vocabulary regarding topics of life and death, due to this being what it has been called in many, many learning resources. We hope to move away from these possibly offensive terms, and towards something we can all discuss without bad memories being brought up.

\begin{itemize}
\item[Live]
  A temporary is considered live after it has been assigned and up until the last use of this temporary.
\item[Live Range]
  The range of expressions in a basic block in which a temporary is live.
\end{itemize}

Liveness analysis is done by keeping track of every use of some calculated value in a linked list of \verb|Use| structures (pronounce like the noun, the 'S' is voiceless). This is done during intermediate representation generation. Effectively, we build our IR with live ranges built-in.

If a temporary is live during the definition of another temporary, the two temporaries are said to \emph{interfere}.

Excerpt from demonstration IR above:
\begin{Verbatim}
  %r0 = global_load("b")   ;; a
  %r1 = global_load("c")   ;; b
  %r2 = add(%r0, %r1)      ;; c
  global_store(%r2, "a")   ;; d
  %r3 = global_load("a")   ;; e
  %r4 = immediate(-1)      ;; f
  %r5 = multiply(%r3, %r4) ;; g
  global_store(%r5, "d")   ;; h
\end{Verbatim}

Live ranges from above excerpt:
\begin{Verbatim}
    r0 r1 r2 r3 r4 r5
  a |
  b |  |
  c |  |  |
  d       |
  e          |
  f          |  |
  g          |  |  |
  h                |
\end{Verbatim}

\chapter{Register Allocation}
\label{sec:codegen-register-allocation}
An IR function may contain a potentially infinite number of temporaries; a physical CPU, by contrast, contains only a limited number
of \textit{physical} or \textit{hardware registers}. Register allocation (RA) is the process of assigning to each temporary a register, and
in such a way that no two interfering temporaries share a register. This is accomplished mainly by means of an \textit{interference graph}.

The interference graph of an IR function is an undirected graph consisting of a set of vertices and a set of edges between those
vertices; every temporary in the function is represented by a vertex, and an edge is drawn between each pair of temporaries that
interfere. The problem of finding an (ideal) allocation of at most $k$ registers to those temporaries is then equivalent to the problem
of finding an (ideal) $k$-colouring of the interference graph.

The process of $k$-colouring a graph $G$ consisting of vertices $V$ and edges $E$ consists in assigning to each vertex $v \in V$ a colour
from $1$ to $k$, such that no vertex $v'$ with $vv' \in E$ (that is, no vertex $v'$ adjacent to $v$) has that same colour.

Or, to put it differently: the process of assigning at most $k$ registers to the temporaries that make up  a function consists in assigning
 to each temporary a register between $1$ and $k$ such that no temporary that interferes with it is assigned that same register.

This is a very high level overview of RA; both the actual implementation and the theory behind it are rather complex, and writing a register
allocator that can account for every last quirk that an architecture may have and which can allocate any correct IR program appropriately
is a fairly complicated endeavour, and the current register allocator is currently still a far cry from that.

Nevertheless, the following sections aim to be a hopefully not too complicated description of the general RA process, as well as the theory
and ideas behind it, the reasons as to why certain things are implemented the way they are, and finally, the shortcomings of the current 
implementation.

\section{PHI Elimination}
The first step of our register allocation algorithm concerns \textit{PHI ($\Phi$) instructions}. Of all the instructions in our IR, the PHI
instruction is easily the most abstract, as it is an instruction that is found in all SSA languages, and yet, there are no physical CPUs
that implement anything remotely similar to it.

As the name might suggest, the central concept of an SSA language like our IR is exactly that: \textit{single static assignment}. Once
created, a temporary can \textit{never} be reassigned, and its value is guaranteed to be the same throughout the entire function. This
works just fine in most cases: most instructions only consume and produce values, but never alter them; even variable reassignment,
can be expressed as a sequence of load and store instruction without any actual reassignment. 

Where it leads to problems is in situations where control flow demands that different values be used depending on what path is taken, 
and while it may theoretically be possible to solve this by employing a temporary store/load to/from memory, we would much rather keep
temporaries in registers for as long as possible and only evict them to memory once we start running out of registers.

The classical example of such a situation is the following \langname{} program (declarations are omitted for the sake of brevity):
\begin{Verbatim}
a := if b { 
  ;; Code that computes ‘c’
  c
} else {
  ;; Code that computes ‘d’
  d
}
\end{Verbatim}

\noindent Compiling this program to IR yields:
\begin{Verbatim}
bb1:
    %b = ... ;; Code that computes ‘%b’
    branch on %b to bb2 else bb3
bb2:
    %c = ... ;; Code that computes ‘%c’
    branch to bb4
bb3:
    %d = ... ;; Code that computes ‘%d’
    branch to bb4
bb4:
    %a = phi [bb1: %c], [bb2: %d]
\end{Verbatim}

\noindent The PHI instruction \verb|%a = phi [bb1: %c], [bb2: %d]| means ‘assign to \verb|%a| the value of \verb|%c| if we reached this block
from \verb|bb1|, and the value of \verb|%d| if we reached this block from \verb|bb2|’; this way, we can assign different values to \verb|%a|
depending on the control flow of the program, without having to assign to \verb|%a| twice.

It is important to realise that a PHI instruction is a very high-level construct. The LLVM documentation quite aptly refers to it as ‘the 
magical mystical PHI node, that can not exist in nature, but can be synthesized in a computer scientist's overactive imagination’. The PHI 
instruction here merely tells us \textit{what} values must be assigned to \verb|%a|—it does not contain or provide us with any information
whatsoever as to \textit{how} this should be accomplished.

Thus, before we can even begin to talk about register allocation proper, we must first convert these PHI nodes into something that can
actually be assigned a register, or several registers. This process is referred to as \textit{PHI lowering} or \textit{PHI elimination}
both here and in the source code of the RA, but it is also often known by other names such as \textit{PHI lifting} or \textit{SSA 
destruction}.

Lowering PHI nodes seems simple at first glance: simply insert a copy from \verb|%c| in \verb|bb2| and from \verb|%d| in \verb|bb3| to 
\verb|%a| before the branch to \verb|bb4|; no code is ultimately generated at the actual location of the PHI instruction itself, but we
still allocate a register for it, and that is the register that we copy \verb|%c| or \verb|%d| into. After register allocation, we may 
end up with something like this:
\begin{Verbatim}
bb1:
    %r1 = ... ;; Code that computes ‘%r1’ (%b)
    branch on %r1 to bb2 else bb3
bb2:
    %r2 = ... ;; Code that computes ‘%r2’ (%c)
    copy %r2 to %r4
    branch to bb4
bb3:
    %r3 = ... ;; Code that computes ‘%r3’ (%d)
    copy %r3 to %r4
    branch to bb4
bb4:
    ;; In this case, the RA has assigned %r4 to %a.
    ;;
    ;; No code is generated for the PHI; the copy
    ;; instructions that we inserted above have
    ;; already placed the right value into %r4 by
    ;; the time we get to this point in the program.
\end{Verbatim}

\noindent Depending on the complexity of the program, a good register allocator may even be able to generate the values of \verb|%b| and
\verb|%c| directly in \verb|%r4|, which would make the copies unnecessary since the values would already be in the right register.

Unfortunately for us, this strategy of simply inserting copies everywhere does not always work. There are two big problems with this
approach, one of which is rather easy to solve (and the solution is implemented in our RA), the other one\ldots{} not so much.

\subsection{Critical Edges}
The first problem occurs when an argument of a PHI instruction originates in a block that has a \textit{critical edge}. A critical edge is
defined as an edge between a block with multiple successors and a block with multiple predecessors (though in our case, it only matters
that the block has multiple successors). 

Consider the following IR program:
\begin{Verbatim}
bb1:
    %b = ...
    %c = ...
    branch on %b to bb2 else bb4
bb2:
    %d = ...
    %e = ...
    branch on %e to bb3 else bb4
bb3:
    ;; Some lengthy computation
    ;; ...
    return
bb4:
    %a = phi [bb1: %c], [bb2: %d]
\end{Verbatim}

\noindent Trying to lower the PHI instruction in \verb|bb4| by simply inserting copies in the blocks above doesn't quite work in this case:
\begin{Verbatim}
bb1:
    %r1 = ...
    %r2 = ...
    copy %r2 to %r4
    branch on %r1 to bb2 else bb4
bb2:
    %r3 = ...
    %r5 = ...
    copy %r3 to %r4
    branch on %r5 to bb3 else bb4 (*)
bb3:
    ;; Some lengthy computation
    ;; ...
    return
bb4:
    ;; Value of ‘%a’ is supposed to be in %r4 here.
\end{Verbatim}

\noindent At first glance, it might seem as though there were nothing wrong with this program; and indeed, it works just fine, so long
as the \verb|(*)| branch instruction always branches to \verb|bb4|. However, what happens if control flow branches to \verb|bb3| instead? 
In that case, we still end up copying \verb|%r3| to \verb|%r4|—even if its value isn't used in \verb|bb3|! This means that \verb|%r4| 
cannot be used at all in \verb|bb3|! 

As to why that is the case, consider the following scenario: assume there is some value \verb|%v| that is computed somewhere before 
\verb|bb2| and which is used in \verb|bb3|, but not in \verb|bb4| or any of its successors. Assume futher that we (incorrectly) allow 
the use of \verb|%r4| in \verb|bb3|; in this case, the RA may decide to assign \verb|%r4| to both \verb|%v| and \verb|%a|. If we consider 
only the semantics of our original IR pre PHI lowering, then this assignment is perfectly reasonable: \verb|%v| is only used up to 
\verb|bb3|, and unused in \verb|bb4| and after, and \verb|%a| is only used in \verb|bb4| and after. The liveness ranges of \verb|%v| and 
\verb|%a| do not overlap, and therefore, they may be assigned the same register.

However, during PHI lowering, we end up inserting a copy instruction from \verb|%r3| to \verb|%r4| in \verb|bb2|. This means that the value
of \verb|%v|, which was held in \verb|%r4| up to this point, is overwritten with \verb|%r3|, \textit{irrespective of what block we branch 
to}. Everything is fine so long as we branch to \verb|bb4|, but if we branch to \verb|bb4|, then we have problem, since \verb|bb4|
expects the value of \verb|%v| to be stored in \verb|%r4|, but we've just overwritten it with the value of \verb|%r3|.

This illustrates that we can't just insert copies into a block that ends with a conditional branch. Fortunately, this problem has a rather
simple solution: if copy insertion is problematic in blocks that contain a conditional branch, then we simply need to insert the copy in a
block that contains only an unconditional branch instead. This can be accomplished by inserting an additional block between \verb|bb2| and
\verb|bb4| and generating the copy instruction in that block:
\begin{Verbatim}
bb1:
    %r1 = ...
    %r2 = ...
    branch on %r1 to bb2 else bb1-copy
bb1-copy:
    copy %r2 to %r4
    branch to bb4
bb2:
    %r3 = ...
    %r5 = ...
    branch on %r5 to bb3 else bb2-copy
bb2-copy
    copy %r3 to %r4
    branch to bb4
bb3:
    ;; Some lengthy computation
    ;; ...
    return
bb4:
    ;; Value of ‘%a’ is can now be in %r4.
\end{Verbatim}

\noindent This way, the right values are always in \verb|%r4| by the time control flow reaches \verb|bb4|, but at the same time, \verb|%r4|
is never clobbered if control flow branches to \verb|bb3| instead. We can now safely use \verb|%r4| for both \verb|%v| and \verb|%a| without
changing the semantics of the program.

\subsection{Multiple PHIs in One Block}

\end{document}
